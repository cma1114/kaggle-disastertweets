{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Finetune encoder with classifier head:\n*     roberta-large\nFinetune decoder with lm head:\n*     distilgpt2\nFinetune encode-decoder with seq2seqlm head (using lora)\n*     flan-t5-large\nTrain classifier on top of embeddings:\n*     gpt2-xl (decoder)\n*     roberta-large (encoder)\n*     openai-ada (decoder; calling api)\nFinetune using OpenAI API\n*     curie\nAsk instruction-tuned LM with no fine tuning\n*     PaLM\n","metadata":{}},{"cell_type":"code","source":"#basic setup\n!pip install --upgrade transformers  \n!pip install --upgrade accelerate\n!pip install datasets\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification #classification head atop base model\nfrom datasets import Dataset,DatasetDict\nfrom transformers import DataCollatorWithPadding\nfrom transformers import create_optimizer\nfrom transformers import TrainingArguments, Trainer\nimport torch\nfrom torch.utils.data import DataLoader\nimport re\nimport os\nimport gc\n\ndef load_data(sep=\"[SEP]\", aug=True, dset='train'):\n    if dset=='train':\n        df = pd.read_csv(\"/kaggle/input/nlpgs-train-cln/train_cln.csv\")\n    else:\n        df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n    #add keyword and location\n    if aug:\n        df['keyword'] = df['keyword'].fillna('unknown')\n        df['location'] = df['location'].fillna('unknown')\n        df['text'] = df.apply(lambda row: f\"{row['text']} {sep} keyword: {row['keyword']} {sep} location: {row['location']}\", axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-06-19T18:02:17.499656Z","iopub.execute_input":"2023-06-19T18:02:17.500342Z","iopub.status.idle":"2023-06-19T18:03:15.766920Z","shell.execute_reply.started":"2023-06-19T18:02:17.500308Z","shell.execute_reply":"2023-06-19T18:03:15.765978Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nCollecting transformers\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.1\n    Uninstalling transformers-4.30.1:\n      Successfully uninstalled transformers-4.30.1\nSuccessfully installed transformers-4.30.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.15.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"#This finetunes encoders with a classification head (using pytorch)\nmodel_name=\"roberta-large\"#\"gpt2-medium\"#\"cardiffnlp/twitter-roberta-base\"#\"roberta-base\"#\"distilbert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ndef tok_func(x): return tokenizer(x[\"text\"], padding=True)\n\ntrain_df = load_data(aug=True, dset='train')\nds = Dataset.from_pandas(train_df)\nds = ds.train_test_split(test_size=0.05, seed=42)\ntok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\ntok_ds = tok_ds.rename_columns({'target':'label'})\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device);\n\ntraining_args = TrainingArguments(\n    output_dir = \"test\",\n    overwrite_output_dir=True,\n    report_to='none',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tok_ds[\"train\"],\n    eval_dataset=tok_ds[\"test\"],\n    data_collator=data_collator,\n)\n \ntrainer.train()\n\n###try it on validation set\nprediction_output = trainer.predict(tok_ds[\"test\"])\n\n# getting predictions and converting to classes\npredictions = prediction_output.predictions\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Prepare the data as a pandas DataFrame\ndata = {\"id\": ds[\"test\"][\"id\"], \"input_text\": ds[\"test\"][\"text\"], \"predicted_label\": predicted_classes, \"true_label\": ds[\"test\"][\"target\"]}\noutput_df = pd.DataFrame(data)\n\n# Save the DataFrame to a CSV file\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_classhead_cln_eval_short.csv', index=False)\n\n####now on test set\ntest_df = load_data(aug=True, dset='test')\nds = Dataset.from_pandas(test_df)\ntok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\nprediction_output = trainer.predict(tok_ds)\npredictions = prediction_output.predictions\npredicted_classes = np.argmax(predictions, axis=1)\ndata = {\"id\": ds[\"id\"], \"input_text\": ds[\"text\"], \"predicted_label\": predicted_classes}\noutput_df = pd.DataFrame(data)\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_classhead_cln_test.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T16:48:34.914357Z","iopub.execute_input":"2023-06-19T16:48:34.914801Z","iopub.status.idle":"2023-06-19T16:59:48.590198Z","shell.execute_reply.started":"2023-06-19T16:48:34.914770Z","shell.execute_reply":"2023-06-19T16:59:48.589331Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4cb29f558847bf8f909c388a4fb3dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2945a4fa4d694ad1b0244fcc1ea78ac2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1239' max='1239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1239/1239 10:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.262497</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.390100</td>\n      <td>0.240739</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.223900</td>\n      <td>0.314278</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221d47d287fe4bdeadf189be854b5ab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"#Causal lm (decoder) fine-tuning\nfrom transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nmodel_name=\"distilgpt2\"#\"gpt2-xl\"#\"gpt2-medium\"#\n\n#load model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n#load and set up tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\nif tokenizer.sep_token is None:\n    sep = '[SEP]'\n    tokenizer.add_special_tokens({'sep_token': sep})\n    model.resize_token_embeddings(len(tokenizer))\nelse:\n    sep = tokenizer.sep_token\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\n\n#load and split the data\ntrain_df = load_data(sep,aug=True,dset='train')\nds = Dataset.from_pandas(train_df)\nds = ds.train_test_split(test_size=0.05, seed=42)\n\n#append labels to training input\ndef format_text(example, train):\n    if train:\n        label = \"Real disaster\" if example['target'] == 1 else \"Not a real disaster\"\n    else:\n        label =\"\"\n    return {'text': f\"Tweet: {example['text']} Label: {label}\"}\nds[\"train\"] = ds[\"train\"].map(lambda example: format_text(example, train=True))\nds[\"test\"] = ds[\"test\"].map(lambda example: format_text(example, train=False))\n\n#tokenize the data\ndef tok_func(x): return tokenizer(x[\"text\"])\ntok_ds = ds.map(tok_func)\ntok_ds = tok_ds.remove_columns(ds['train'].column_names)\n\n#set params\nbatch_size = 8\nlearning_rate=3e-4#2e-5\nnum_train_epochs=5\nweight_decay=0.01\n\n#set up the data collator and loaders\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntrain_dataloader = DataLoader(tok_ds['train'], shuffle=True, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\neval_dataloader = DataLoader(tok_ds['test'], collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n\n#load optimizer and lr_scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_train_epochs),\n)\n\n#do training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference! \nfor epoch in range(0,num_train_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"epoch: {epoch}, train_ppl: {train_ppl}, train_epoch_loss: {train_epoch_loss}\")\n\n#do eval\nmodel.config.use_cache = True \ndef label_match(input_str):\n    match = re.search(\" label\\s*:\\s*.*\", input_str.lower())  \n    if match is not None:\n        label_part = match.group()\n        if \"label: not a real disaster\" in label_part:\n            return 0\n        elif \"label: real disaster\" in label_part:\n            return 1\n    return None  \n\n###run it on validation set\nmodel.eval()\ndecoded_outputs = []\nfor i, batch in enumerate(tqdm(eval_dataloader)):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**batch, pad_token_id=tokenizer.pad_token_id, max_new_tokens=20)\n    \n    outtexts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_outputs.extend([label_match(outtext) for outtext in outtexts])\n\n# prepare the data as a pandas DataFrame and output to csv\ndata = {\"id\": ds[\"test\"][\"id\"], \"input_text\": ds[\"test\"][\"text\"], \"predicted_label\": decoded_outputs, \"true_label\": ds[\"test\"][\"target\"]}\noutput_df = pd.DataFrame(data)\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_cln_eval_short.csv', index=False)\n\n###now do test\ntest_df = load_data(sep,aug=True,dset='test')\nds = Dataset.from_pandas(test_df)\nds = ds.map(lambda example: format_text(example, train=False))\ntok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\ntest_dataloader = DataLoader(tok_ds, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\ndecoded_outputs = []\nfor i, batch in enumerate(tqdm(test_dataloader)):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**batch, pad_token_id=tokenizer.pad_token_id, max_new_tokens=20)\n    \n    outtexts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_outputs.extend([label_match(outtext) for outtext in outtexts])\n\ndata = {\"id\": ds[\"id\"], \"input_text\": ds[\"text\"], \"predicted_label\": decoded_outputs}\noutput_df = pd.DataFrame(data)\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_cln_test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T17:03:42.140538Z","iopub.execute_input":"2023-06-19T17:03:42.140914Z","iopub.status.idle":"2023-06-19T17:09:10.392477Z","shell.execute_reply.started":"2023-06-19T17:03:42.140883Z","shell.execute_reply":"2023-06-19T17:09:10.389373Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16178d74dc564883b11d3c97e4abdb81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57adb179838c484ba2c2b3bc0905b11d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ca3d644c8b403fbf476d090c8793a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"804ff5aac9894ad0a1ec84d07915a945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e43b3db155aa4ad0a4547d194dcefa89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd83757dbdc4e32a6d78f9bab305480"}},"metadata":{}},{"name":"stderr","text":"Using sep_token, but it is not set yet.\nUsing pad_token, but it is not set yet.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfde9377e112432cb427a7dc5577cdc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/347 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e194a1005e4078b86a022a1439134a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6593 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff9a46e83d542c3926b39c3909be350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/347 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e14b5891d25e426eb64e7d5fe961231e"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/825 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n100%|██████████| 825/825 [00:49<00:00, 16.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, train_ppl: 33.99455261230469, train_epoch_loss: 3.526200294494629\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:48<00:00, 16.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 1, train_ppl: 12.650788307189941, train_epoch_loss: 2.537719488143921\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:49<00:00, 16.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 2, train_ppl: 8.9413423538208, train_epoch_loss: 2.190685749053955\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:48<00:00, 16.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 3, train_ppl: 6.6375017166137695, train_epoch_loss: 1.8927356004714966\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:49<00:00, 16.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 4, train_ppl: 5.212894916534424, train_epoch_loss: 1.6511353254318237\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:08<00:00,  5.07it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3263 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1bb122a90d4dd29bb29c651ecc6a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f56632491f4474b39c7c34629731ad"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 408/408 [00:58<00:00,  7.01it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Embeddings + logreg classifier \ndef get_model_embeddings(texts, model, tokenizer, batch_size=8, encoder=False):\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n    dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()  # Put the model in eval mode\n\n    embeddings = []\n    with torch.no_grad():\n        for i, (input_ids, attention_mask) in enumerate(data_loader):\n            print(\"i=\",i)\n            # Feed our sequences to the model\n            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n            \n            mean_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n            if encoder:\n                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n            else:\n                cls_embeddings = outputs.last_hidden_state[:, -1, :].cpu().numpy()\n\n            # Store both mean and last token embeddings\n            embeddings.extend(list(zip(mean_embeddings, cls_embeddings)))\n\n    return embeddings\n\ndef ClassifyEmbeddings(targets, embeddings_train, embeddings_eval, embeddings_test=None):\n    from sklearn.linear_model import LogisticRegression\n\n    clf = LogisticRegression(max_iter=1000)\n\n    clf.fit(embeddings_train, np.array(targets))\n\n    y_val_pred = clf.predict(embeddings_eval)\n    \n    if embeddings_test is not None:\n        y_test_pred = clf.predict(embeddings_test)\n    else:\n        y_test_pred = None\n    return y_val_pred, y_test_pred\n\n#load and split the data\ntrain_df = load_data(aug=True,dset='train')\nds = Dataset.from_pandas(train_df)\nds = ds.train_test_split(test_size=0.05, seed=42)\ntest_df = load_data(aug=True,dset='test')\ntest_ds = Dataset.from_pandas(test_df)\n\nmodel_name=\"roberta-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nembeddings = {}\nembeddings['train'] = get_model_embeddings(ds['train']['text'], model, tokenizer, batch_size=16, encoder=True)\nembeddings['eval'] = get_model_embeddings(ds['test']['text'], model, tokenizer, batch_size=16, encoder=True)\nembeddings['test'] = get_model_embeddings(test_ds['text'], model, tokenizer, batch_size=16, encoder=True)\nmean_embeddings_train, cls_embeddings_train = zip(*embeddings['train'])\nmean_embeddings_eval, cls_embeddings_eval = zip(*embeddings['eval'])\nmean_embeddings_test, cls_embeddings_test = zip(*embeddings['test'])\ny_val_pred, y_test_pred = ClassifyEmbeddings(ds[\"train\"][\"target\"], cls_embeddings_train, cls_embeddings_eval, cls_embeddings_test)\noutput_df = pd.DataFrame({'id': ds['test']['id'],'input_text': ds['test'][\"text\"],'predicted_label': y_val_pred,\n        'true_label': ds[\"test\"][\"target\"]})\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_cls_logreg_cln_eval_short.csv\", index=False)\noutput_df = pd.DataFrame({'id': test_ds['id'],'input_text': test_ds[\"text\"],'predicted_label': y_test_pred})\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_cls_logreg_cln_test.csv\", index=False)\n\nmodel = None\ntokenizer=None\ngc.collect()\ntorch.cuda.empty_cache()\n\nmodel_name=\"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\nembeddings = {}\nembeddings['train'] = get_model_embeddings(ds['train']['text'], model, tokenizer, batch_size=16, encoder=False)\nembeddings['eval'] = get_model_embeddings(ds['test']['text'], model, tokenizer, batch_size=16, encoder=False)\nembeddings['test'] = get_model_embeddings(test_ds['text'], model, tokenizer, batch_size=16, encoder=False)\nmean_embeddings_train, cls_embeddings_train = zip(*embeddings['train'])\nmean_embeddings_eval, cls_embeddings_eval = zip(*embeddings['eval'])\nmean_embeddings_test, cls_embeddings_test = zip(*embeddings['test'])\ny_val_pred, y_test_pred = ClassifyEmbeddings(ds[\"train\"][\"target\"], cls_embeddings_train, cls_embeddings_eval, cls_embeddings_test)\noutput_df = pd.DataFrame({'id': ds['test']['id'],'input_text': ds['test'][\"text\"],'predicted_label': y_val_pred,\n        'true_label': ds[\"test\"][\"target\"]})\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_last_logreg_cln_eval_short.csv\", index=False)\noutput_df = pd.DataFrame({'id': test_ds['id'],'input_text': test_ds[\"text\"],'predicted_label': y_test_pred})\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_last_logreg_cln_test.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T18:03:38.868727Z","iopub.execute_input":"2023-06-19T18:03:38.869081Z","iopub.status.idle":"2023-06-19T18:15:44.730413Z","shell.execute_reply.started":"2023-06-19T18:03:38.869052Z","shell.execute_reply":"2023-06-19T18:15:44.728951Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8026e55d4d794b0581c815637f687fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c39feba81c40b990116f1493b3fb19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb5f07290e340f9b41ce6d927d51c4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57fad4c5423f4566950578aac1353a84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9014701b88254101bd1954ab5315bec3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"i= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 22\ni= 23\ni= 24\ni= 25\ni= 26\ni= 27\ni= 28\ni= 29\ni= 30\ni= 31\ni= 32\ni= 33\ni= 34\ni= 35\ni= 36\ni= 37\ni= 38\ni= 39\ni= 40\ni= 41\ni= 42\ni= 43\ni= 44\ni= 45\ni= 46\ni= 47\ni= 48\ni= 49\ni= 50\ni= 51\ni= 52\ni= 53\ni= 54\ni= 55\ni= 56\ni= 57\ni= 58\ni= 59\ni= 60\ni= 61\ni= 62\ni= 63\ni= 64\ni= 65\ni= 66\ni= 67\ni= 68\ni= 69\ni= 70\ni= 71\ni= 72\ni= 73\ni= 74\ni= 75\ni= 76\ni= 77\ni= 78\ni= 79\ni= 80\ni= 81\ni= 82\ni= 83\ni= 84\ni= 85\ni= 86\ni= 87\ni= 88\ni= 89\ni= 90\ni= 91\ni= 92\ni= 93\ni= 94\ni= 95\ni= 96\ni= 97\ni= 98\ni= 99\ni= 100\ni= 101\ni= 102\ni= 103\ni= 104\ni= 105\ni= 106\ni= 107\ni= 108\ni= 109\ni= 110\ni= 111\ni= 112\ni= 113\ni= 114\ni= 115\ni= 116\ni= 117\ni= 118\ni= 119\ni= 120\ni= 121\ni= 122\ni= 123\ni= 124\ni= 125\ni= 126\ni= 127\ni= 128\ni= 129\ni= 130\ni= 131\ni= 132\ni= 133\ni= 134\ni= 135\ni= 136\ni= 137\ni= 138\ni= 139\ni= 140\ni= 141\ni= 142\ni= 143\ni= 144\ni= 145\ni= 146\ni= 147\ni= 148\ni= 149\ni= 150\ni= 151\ni= 152\ni= 153\ni= 154\ni= 155\ni= 156\ni= 157\ni= 158\ni= 159\ni= 160\ni= 161\ni= 162\ni= 163\ni= 164\ni= 165\ni= 166\ni= 167\ni= 168\ni= 169\ni= 170\ni= 171\ni= 172\ni= 173\ni= 174\ni= 175\ni= 176\ni= 177\ni= 178\ni= 179\ni= 180\ni= 181\ni= 182\ni= 183\ni= 184\ni= 185\ni= 186\ni= 187\ni= 188\ni= 189\ni= 190\ni= 191\ni= 192\ni= 193\ni= 194\ni= 195\ni= 196\ni= 197\ni= 198\ni= 199\ni= 200\ni= 201\ni= 202\ni= 203\ni= 204\ni= 205\ni= 206\ni= 207\ni= 208\ni= 209\ni= 210\ni= 211\ni= 212\ni= 213\ni= 214\ni= 215\ni= 216\ni= 217\ni= 218\ni= 219\ni= 220\ni= 221\ni= 222\ni= 223\ni= 224\ni= 225\ni= 226\ni= 227\ni= 228\ni= 229\ni= 230\ni= 231\ni= 232\ni= 233\ni= 234\ni= 235\ni= 236\ni= 237\ni= 238\ni= 239\ni= 240\ni= 241\ni= 242\ni= 243\ni= 244\ni= 245\ni= 246\ni= 247\ni= 248\ni= 249\ni= 250\ni= 251\ni= 252\ni= 253\ni= 254\ni= 255\ni= 256\ni= 257\ni= 258\ni= 259\ni= 260\ni= 261\ni= 262\ni= 263\ni= 264\ni= 265\ni= 266\ni= 267\ni= 268\ni= 269\ni= 270\ni= 271\ni= 272\ni= 273\ni= 274\ni= 275\ni= 276\ni= 277\ni= 278\ni= 279\ni= 280\ni= 281\ni= 282\ni= 283\ni= 284\ni= 285\ni= 286\ni= 287\ni= 288\ni= 289\ni= 290\ni= 291\ni= 292\ni= 293\ni= 294\ni= 295\ni= 296\ni= 297\ni= 298\ni= 299\ni= 300\ni= 301\ni= 302\ni= 303\ni= 304\ni= 305\ni= 306\ni= 307\ni= 308\ni= 309\ni= 310\ni= 311\ni= 312\ni= 313\ni= 314\ni= 315\ni= 316\ni= 317\ni= 318\ni= 319\ni= 320\ni= 321\ni= 322\ni= 323\ni= 324\ni= 325\ni= 326\ni= 327\ni= 328\ni= 329\ni= 330\ni= 331\ni= 332\ni= 333\ni= 334\ni= 335\ni= 336\ni= 337\ni= 338\ni= 339\ni= 340\ni= 341\ni= 342\ni= 343\ni= 344\ni= 345\ni= 346\ni= 347\ni= 348\ni= 349\ni= 350\ni= 351\ni= 352\ni= 353\ni= 354\ni= 355\ni= 356\ni= 357\ni= 358\ni= 359\ni= 360\ni= 361\ni= 362\ni= 363\ni= 364\ni= 365\ni= 366\ni= 367\ni= 368\ni= 369\ni= 370\ni= 371\ni= 372\ni= 373\ni= 374\ni= 375\ni= 376\ni= 377\ni= 378\ni= 379\ni= 380\ni= 381\ni= 382\ni= 383\ni= 384\ni= 385\ni= 386\ni= 387\ni= 388\ni= 389\ni= 390\ni= 391\ni= 392\ni= 393\ni= 394\ni= 395\ni= 396\ni= 397\ni= 398\ni= 399\ni= 400\ni= 401\ni= 402\ni= 403\ni= 404\ni= 405\ni= 406\ni= 407\ni= 408\ni= 409\ni= 410\ni= 411\ni= 412\ni= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 22\ni= 23\ni= 24\ni= 25\ni= 26\ni= 27\ni= 28\ni= 29\ni= 30\ni= 31\ni= 32\ni= 33\ni= 34\ni= 35\ni= 36\ni= 37\ni= 38\ni= 39\ni= 40\ni= 41\ni= 42\ni= 43\ni= 44\ni= 45\ni= 46\ni= 47\ni= 48\ni= 49\ni= 50\ni= 51\ni= 52\ni= 53\ni= 54\ni= 55\ni= 56\ni= 57\ni= 58\ni= 59\ni= 60\ni= 61\ni= 62\ni= 63\ni= 64\ni= 65\ni= 66\ni= 67\ni= 68\ni= 69\ni= 70\ni= 71\ni= 72\ni= 73\ni= 74\ni= 75\ni= 76\ni= 77\ni= 78\ni= 79\ni= 80\ni= 81\ni= 82\ni= 83\ni= 84\ni= 85\ni= 86\ni= 87\ni= 88\ni= 89\ni= 90\ni= 91\ni= 92\ni= 93\ni= 94\ni= 95\ni= 96\ni= 97\ni= 98\ni= 99\ni= 100\ni= 101\ni= 102\ni= 103\ni= 104\ni= 105\ni= 106\ni= 107\ni= 108\ni= 109\ni= 110\ni= 111\ni= 112\ni= 113\ni= 114\ni= 115\ni= 116\ni= 117\ni= 118\ni= 119\ni= 120\ni= 121\ni= 122\ni= 123\ni= 124\ni= 125\ni= 126\ni= 127\ni= 128\ni= 129\ni= 130\ni= 131\ni= 132\ni= 133\ni= 134\ni= 135\ni= 136\ni= 137\ni= 138\ni= 139\ni= 140\ni= 141\ni= 142\ni= 143\ni= 144\ni= 145\ni= 146\ni= 147\ni= 148\ni= 149\ni= 150\ni= 151\ni= 152\ni= 153\ni= 154\ni= 155\ni= 156\ni= 157\ni= 158\ni= 159\ni= 160\ni= 161\ni= 162\ni= 163\ni= 164\ni= 165\ni= 166\ni= 167\ni= 168\ni= 169\ni= 170\ni= 171\ni= 172\ni= 173\ni= 174\ni= 175\ni= 176\ni= 177\ni= 178\ni= 179\ni= 180\ni= 181\ni= 182\ni= 183\ni= 184\ni= 185\ni= 186\ni= 187\ni= 188\ni= 189\ni= 190\ni= 191\ni= 192\ni= 193\ni= 194\ni= 195\ni= 196\ni= 197\ni= 198\ni= 199\ni= 200\ni= 201\ni= 202\ni= 203\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0c2eaae3bb42a09ed607341bad5e69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545cd538dfbb483a9b3889d85cc20eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb71f7c29120482986df02ce125fbb6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b56670d67f460b8c0e9d508e1d12df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ed7388dc0b43f4b61d38cac8b69bbe"}},"metadata":{}},{"name":"stderr","text":"Using pad_token, but it is not set yet.\n","output_type":"stream"},{"name":"stdout","text":"i= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 22\ni= 23\ni= 24\ni= 25\ni= 26\ni= 27\ni= 28\ni= 29\ni= 30\ni= 31\ni= 32\ni= 33\ni= 34\ni= 35\ni= 36\ni= 37\ni= 38\ni= 39\ni= 40\ni= 41\ni= 42\ni= 43\ni= 44\ni= 45\ni= 46\ni= 47\ni= 48\ni= 49\ni= 50\ni= 51\ni= 52\ni= 53\ni= 54\ni= 55\ni= 56\ni= 57\ni= 58\ni= 59\ni= 60\ni= 61\ni= 62\ni= 63\ni= 64\ni= 65\ni= 66\ni= 67\ni= 68\ni= 69\ni= 70\ni= 71\ni= 72\ni= 73\ni= 74\ni= 75\ni= 76\ni= 77\ni= 78\ni= 79\ni= 80\ni= 81\ni= 82\ni= 83\ni= 84\ni= 85\ni= 86\ni= 87\ni= 88\ni= 89\ni= 90\ni= 91\ni= 92\ni= 93\ni= 94\ni= 95\ni= 96\ni= 97\ni= 98\ni= 99\ni= 100\ni= 101\ni= 102\ni= 103\ni= 104\ni= 105\ni= 106\ni= 107\ni= 108\ni= 109\ni= 110\ni= 111\ni= 112\ni= 113\ni= 114\ni= 115\ni= 116\ni= 117\ni= 118\ni= 119\ni= 120\ni= 121\ni= 122\ni= 123\ni= 124\ni= 125\ni= 126\ni= 127\ni= 128\ni= 129\ni= 130\ni= 131\ni= 132\ni= 133\ni= 134\ni= 135\ni= 136\ni= 137\ni= 138\ni= 139\ni= 140\ni= 141\ni= 142\ni= 143\ni= 144\ni= 145\ni= 146\ni= 147\ni= 148\ni= 149\ni= 150\ni= 151\ni= 152\ni= 153\ni= 154\ni= 155\ni= 156\ni= 157\ni= 158\ni= 159\ni= 160\ni= 161\ni= 162\ni= 163\ni= 164\ni= 165\ni= 166\ni= 167\ni= 168\ni= 169\ni= 170\ni= 171\ni= 172\ni= 173\ni= 174\ni= 175\ni= 176\ni= 177\ni= 178\ni= 179\ni= 180\ni= 181\ni= 182\ni= 183\ni= 184\ni= 185\ni= 186\ni= 187\ni= 188\ni= 189\ni= 190\ni= 191\ni= 192\ni= 193\ni= 194\ni= 195\ni= 196\ni= 197\ni= 198\ni= 199\ni= 200\ni= 201\ni= 202\ni= 203\ni= 204\ni= 205\ni= 206\ni= 207\ni= 208\ni= 209\ni= 210\ni= 211\ni= 212\ni= 213\ni= 214\ni= 215\ni= 216\ni= 217\ni= 218\ni= 219\ni= 220\ni= 221\ni= 222\ni= 223\ni= 224\ni= 225\ni= 226\ni= 227\ni= 228\ni= 229\ni= 230\ni= 231\ni= 232\ni= 233\ni= 234\ni= 235\ni= 236\ni= 237\ni= 238\ni= 239\ni= 240\ni= 241\ni= 242\ni= 243\ni= 244\ni= 245\ni= 246\ni= 247\ni= 248\ni= 249\ni= 250\ni= 251\ni= 252\ni= 253\ni= 254\ni= 255\ni= 256\ni= 257\ni= 258\ni= 259\ni= 260\ni= 261\ni= 262\ni= 263\ni= 264\ni= 265\ni= 266\ni= 267\ni= 268\ni= 269\ni= 270\ni= 271\ni= 272\ni= 273\ni= 274\ni= 275\ni= 276\ni= 277\ni= 278\ni= 279\ni= 280\ni= 281\ni= 282\ni= 283\ni= 284\ni= 285\ni= 286\ni= 287\ni= 288\ni= 289\ni= 290\ni= 291\ni= 292\ni= 293\ni= 294\ni= 295\ni= 296\ni= 297\ni= 298\ni= 299\ni= 300\ni= 301\ni= 302\ni= 303\ni= 304\ni= 305\ni= 306\ni= 307\ni= 308\ni= 309\ni= 310\ni= 311\ni= 312\ni= 313\ni= 314\ni= 315\ni= 316\ni= 317\ni= 318\ni= 319\ni= 320\ni= 321\ni= 322\ni= 323\ni= 324\ni= 325\ni= 326\ni= 327\ni= 328\ni= 329\ni= 330\ni= 331\ni= 332\ni= 333\ni= 334\ni= 335\ni= 336\ni= 337\ni= 338\ni= 339\ni= 340\ni= 341\ni= 342\ni= 343\ni= 344\ni= 345\ni= 346\ni= 347\ni= 348\ni= 349\ni= 350\ni= 351\ni= 352\ni= 353\ni= 354\ni= 355\ni= 356\ni= 357\ni= 358\ni= 359\ni= 360\ni= 361\ni= 362\ni= 363\ni= 364\ni= 365\ni= 366\ni= 367\ni= 368\ni= 369\ni= 370\ni= 371\ni= 372\ni= 373\ni= 374\ni= 375\ni= 376\ni= 377\ni= 378\ni= 379\ni= 380\ni= 381\ni= 382\ni= 383\ni= 384\ni= 385\ni= 386\ni= 387\ni= 388\ni= 389\ni= 390\ni= 391\ni= 392\ni= 393\ni= 394\ni= 395\ni= 396\ni= 397\ni= 398\ni= 399\ni= 400\ni= 401\ni= 402\ni= 403\ni= 404\ni= 405\ni= 406\ni= 407\ni= 408\ni= 409\ni= 410\ni= 411\ni= 412\ni= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 0\ni= 1\ni= 2\ni= 3\ni= 4\ni= 5\ni= 6\ni= 7\ni= 8\ni= 9\ni= 10\ni= 11\ni= 12\ni= 13\ni= 14\ni= 15\ni= 16\ni= 17\ni= 18\ni= 19\ni= 20\ni= 21\ni= 22\ni= 23\ni= 24\ni= 25\ni= 26\ni= 27\ni= 28\ni= 29\ni= 30\ni= 31\ni= 32\ni= 33\ni= 34\ni= 35\ni= 36\ni= 37\ni= 38\ni= 39\ni= 40\ni= 41\ni= 42\ni= 43\ni= 44\ni= 45\ni= 46\ni= 47\ni= 48\ni= 49\ni= 50\ni= 51\ni= 52\ni= 53\ni= 54\ni= 55\ni= 56\ni= 57\ni= 58\ni= 59\ni= 60\ni= 61\ni= 62\ni= 63\ni= 64\ni= 65\ni= 66\ni= 67\ni= 68\ni= 69\ni= 70\ni= 71\ni= 72\ni= 73\ni= 74\ni= 75\ni= 76\ni= 77\ni= 78\ni= 79\ni= 80\ni= 81\ni= 82\ni= 83\ni= 84\ni= 85\ni= 86\ni= 87\ni= 88\ni= 89\ni= 90\ni= 91\ni= 92\ni= 93\ni= 94\ni= 95\ni= 96\ni= 97\ni= 98\ni= 99\ni= 100\ni= 101\ni= 102\ni= 103\ni= 104\ni= 105\ni= 106\ni= 107\ni= 108\ni= 109\ni= 110\ni= 111\ni= 112\ni= 113\ni= 114\ni= 115\ni= 116\ni= 117\ni= 118\ni= 119\ni= 120\ni= 121\ni= 122\ni= 123\ni= 124\ni= 125\ni= 126\ni= 127\ni= 128\ni= 129\ni= 130\ni= 131\ni= 132\ni= 133\ni= 134\ni= 135\ni= 136\ni= 137\ni= 138\ni= 139\ni= 140\ni= 141\ni= 142\ni= 143\ni= 144\ni= 145\ni= 146\ni= 147\ni= 148\ni= 149\ni= 150\ni= 151\ni= 152\ni= 153\ni= 154\ni= 155\ni= 156\ni= 157\ni= 158\ni= 159\ni= 160\ni= 161\ni= 162\ni= 163\ni= 164\ni= 165\ni= 166\ni= 167\ni= 168\ni= 169\ni= 170\ni= 171\ni= 172\ni= 173\ni= 174\ni= 175\ni= 176\ni= 177\ni= 178\ni= 179\ni= 180\ni= 181\ni= 182\ni= 183\ni= 184\ni= 185\ni= 186\ni= 187\ni= 188\ni= 189\ni= 190\ni= 191\ni= 192\ni= 193\ni= 194\ni= 195\ni= 196\ni= 197\ni= 198\ni= 199\ni= 200\ni= 201\ni= 202\ni= 203\n","output_type":"stream"}]},{"cell_type":"code","source":"#finetune encoder-decoder with lora - SEQ_2_SEQ_LM\n!pip install peft\nmodel_name=\"google/flan-t5-large\"#\"distilbert-base-uncased\"#\"cardiffnlp/twitter-roberta-base\"#\"roberta-base\"#\"bigscience/bloom-560m\"#\"roberta-large\"#\npeft_model_id = \"cackerman/\"+model_name.rsplit('/', 1)[-1]+\"_aug_LORA_SEQ_2_SEQ_LM\"\nfrom peft import get_peft_model, LoraConfig, TaskType, get_peft_config, get_peft_model_state_dict\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\n\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, #SEQ_CLS\n                         lora_dropout=0.1,target_modules=[\"q\", \"v\"])#for t5#)#, target_modules=[\"q_lin\", \"v_lin\"])#for distilbert (https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#L202)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)#, device_map='auto', torch_dtype=torch.float16)\nmodel = get_peft_model(model, peft_config)\n###model.print_trainable_parameters()\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.sep_token is None:\n    tokenizer.sep_token = tokenizer.eos_token\n    # resize model embedding to match new tokenizer\n    model.resize_token_embeddings(len(tokenizer))\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = model.config.eos_token_id\n    # resize model embedding to match new tokenizer\n    model.resize_token_embeddings(len(tokenizer))\nif \"gpt\" in model_name:\n    tokenizer.padding_side = 'left'\n\ntext_column = \"text\"\nlabel_column = \"text_label\"\nmax_length = 128#64\nlr = 3e-4\nnum_epochs = 3\nbatch_size = 8\n\ntrain_df = load_data(tokenizer.sep_token,True,'train')\nds = Dataset.from_pandas(train_df)\nds = ds.train_test_split(test_size=0.05, seed=42)\nclasses = ['Not a real disaster','Real disaster']\nds = ds.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"target\"]]},\n    batched=True,\n    num_proc=1,\n)\ntarget_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = tokenizer(targets, max_length=target_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    # Convert all torch.Tensors to lists or numpy arrays\n    for key in model_inputs.keys():\n        model_inputs[key] = model_inputs[key].numpy() # or .tolist()\n    model_inputs[\"labels\"] = labels.numpy() # or .tolist()\n    return model_inputs\n\nprocessed_datasets = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n\ntrain_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"test\"]\n    \ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n                \n###checkpoint = torch.load(save_path)\n###model.load_state_dict(checkpoint[\"model_state_dict\"])\nprint(model.print_trainable_parameters())\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor epoch in range(0,num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n#        if epoch==4 and step <= 704:\n#          continue\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        # save model every save_steps\n#        if step % save_steps == 0 and step > 0:\n#          torch.save({\n#              \"model_state_dict\": model.state_dict(),\n#              \"optimizer_state_dict\": optimizer.state_dict(),\n#              \"scheduler_state_dict\": lr_scheduler.state_dict()\n#          }, save_path)\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"epoch: {epoch}, train_ppl: {train_ppl}, train_epoch_loss: {train_epoch_loss}, eval_ppl: {eval_ppl}, eval_epoch_loss: {eval_epoch_loss}\")\n\ncorrect = 0\ntotal = 0\nfor pred, true in zip(eval_preds, ds[\"test\"][\"text_label\"]):\n    if pred.strip() == true.strip():\n        correct += 1\n    total += 1\naccuracy = correct / total * 100\nprint(f\"{accuracy} % on the evaluation dataset\")\nprint(f\"{eval_preds[:10]}\")\nprint(f\"{ds['test']['text_label'][:10]}\")\n\nimport re\n\n###run on eval set\ndecoded_outputs = []\ndef label_mapping(input_str):\n    if input_str == \"Not a real disaster\":\n        return 0\n    elif input_str == \"Real disaster\":\n        return 1\n    return\nfor i, batch in enumerate(eval_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    input_ids = batch['input_ids'].to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids=input_ids, max_new_tokens=10)\n        outtext=tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n        if i==0:\n            print(\"inputs=\",tokenizer.batch_decode(input_ids.detach().cpu().numpy(), skip_special_tokens=True))\n            print(\"outtext=\",outtext)\n        decoded_outputs.extend(label_mapping(str) for str in outtext)\n\ntargets = ds[\"test\"][\"target\"]\ninputs = ds[\"test\"][\"text\"]\n\ndata = {\"id\": ds[\"test\"][\"id\"], \"input_text\": inputs, \"predicted_label\": decoded_outputs, \"true_label\": targets}\noutput_df = pd.DataFrame(data)\n\n# Save the DataFrame to a CSV file\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetunedlora_classhead_cln_eval_short.csv', index=False)\n\n\n###now do test set\ntest_df = load_data(tokenizer.sep_token,True,'test')\ntest_ds = Dataset.from_pandas(test_df)\ndef tok_func(x): return tokenizer(x[\"text\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\ntok_ds = test_ds.map(tok_func)\ntok_ds = tok_ds.remove_columns(test_ds.column_names)\ntest_dataloader = DataLoader(tok_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n\ndecoded_outputs = []\nfor i, batch in enumerate(test_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    input_ids = batch['input_ids'].to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids=input_ids, max_new_tokens=10)\n        outtext=tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n        if i==0:\n            print(\"inputs=\",tokenizer.batch_decode(input_ids.detach().cpu().numpy(), skip_special_tokens=True))\n            print(\"outtext=\",outtext)\n        decoded_outputs.extend(label_mapping(str) for str in outtext)\n\ndata = {\"id\": test_ds[\"id\"], \"input_text\": test_ds['text'], \"predicted_label\": decoded_outputs}\noutput_df = pd.DataFrame(data)\noutput_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetunedlora_classhead_cln_test.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}