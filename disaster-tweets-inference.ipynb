{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-19T18:02:17.500342Z","iopub.status.busy":"2023-06-19T18:02:17.499656Z","iopub.status.idle":"2023-06-19T18:03:15.766920Z","shell.execute_reply":"2023-06-19T18:03:15.765978Z","shell.execute_reply.started":"2023-06-19T18:02:17.500308Z"},"trusted":true},"outputs":[],"source":["#basic setup\n","!pip install --upgrade transformers  \n","!pip install --upgrade accelerate\n","!pip install datasets\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification #classification head atop base model\n","from datasets import Dataset,DatasetDict\n","from transformers import DataCollatorWithPadding\n","from transformers import create_optimizer\n","from transformers import TrainingArguments, Trainer\n","import torch\n","from torch.utils.data import DataLoader\n","import re\n","import os\n","import gc\n","\n","def load_data(sep=\"[SEP]\", aug=True, dset='train'):\n","    if dset=='train':\n","        df = pd.read_csv(\"/kaggle/input/nlpgs-train-cln/train_cln.csv\")\n","    else:\n","        df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n","    #add keyword and location\n","    if aug:\n","        df['keyword'] = df['keyword'].fillna('unknown')\n","        df['location'] = df['location'].fillna('unknown')\n","        df['text'] = df.apply(lambda row: f\"{row['text']} {sep} keyword: {row['keyword']} {sep} location: {row['location']}\", axis=1)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-19T16:48:34.914801Z","iopub.status.busy":"2023-06-19T16:48:34.914357Z","iopub.status.idle":"2023-06-19T16:59:48.590198Z","shell.execute_reply":"2023-06-19T16:59:48.589331Z","shell.execute_reply.started":"2023-06-19T16:48:34.914770Z"},"trusted":true},"outputs":[],"source":["#This finetunes encoders with a classification head (using pytorch)\n","model_name=\"roberta-large\"#\"gpt2-medium\"#\"cardiffnlp/twitter-roberta-base\"#\"roberta-base\"#\"distilbert-base-uncased\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","def tok_func(x): return tokenizer(x[\"text\"], padding=True)\n","\n","train_df = load_data(aug=True, dset='train')\n","ds = Dataset.from_pandas(train_df)\n","ds = ds.train_test_split(test_size=0.05, seed=42)\n","tok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\n","tok_ds = tok_ds.rename_columns({'target':'label'})\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device);\n","\n","training_args = TrainingArguments(\n","    output_dir = \"test\",\n","    overwrite_output_dir=True,\n","    report_to='none',\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tok_ds[\"train\"],\n","    eval_dataset=tok_ds[\"test\"],\n","    data_collator=data_collator,\n",")\n"," \n","trainer.train()\n","\n","###try it on validation set\n","prediction_output = trainer.predict(tok_ds[\"test\"])\n","\n","# getting predictions and converting to classes\n","predictions = prediction_output.predictions\n","predicted_classes = np.argmax(predictions, axis=1)\n","\n","# Prepare the data as a pandas DataFrame\n","data = {\"id\": ds[\"test\"][\"id\"], \"input_text\": ds[\"test\"][\"text\"], \"predicted_label\": predicted_classes, \"true_label\": ds[\"test\"][\"target\"]}\n","output_df = pd.DataFrame(data)\n","\n","# Save the DataFrame to a CSV file\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_classhead_cln_eval_short.csv', index=False)\n","\n","####now on test set\n","test_df = load_data(aug=True, dset='test')\n","ds = Dataset.from_pandas(test_df)\n","tok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\n","prediction_output = trainer.predict(tok_ds)\n","predictions = prediction_output.predictions\n","predicted_classes = np.argmax(predictions, axis=1)\n","data = {\"id\": ds[\"id\"], \"input_text\": ds[\"text\"], \"predicted_label\": predicted_classes}\n","output_df = pd.DataFrame(data)\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_classhead_cln_test.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-19T17:03:42.140914Z","iopub.status.busy":"2023-06-19T17:03:42.140538Z","iopub.status.idle":"2023-06-19T17:09:10.392477Z","shell.execute_reply":"2023-06-19T17:09:10.389373Z","shell.execute_reply.started":"2023-06-19T17:03:42.140883Z"},"trusted":true},"outputs":[],"source":["#Causal lm (decoder) fine-tuning\n","from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","model_name=\"distilgpt2\"#\"gpt2-xl\"#\"gpt2-medium\"#\n","\n","#load model\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","#load and set up tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n","if tokenizer.sep_token is None:\n","    sep = '[SEP]'\n","    tokenizer.add_special_tokens({'sep_token': sep})\n","    model.resize_token_embeddings(len(tokenizer))\n","else:\n","    sep = tokenizer.sep_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","#load and split the data\n","train_df = load_data(sep,aug=True,dset='train')\n","ds = Dataset.from_pandas(train_df)\n","ds = ds.train_test_split(test_size=0.05, seed=42)\n","\n","#append labels to training input\n","def format_text(example, train):\n","    if train:\n","        label = \"Real disaster\" if example['target'] == 1 else \"Not a real disaster\"\n","    else:\n","        label =\"\"\n","    return {'text': f\"Tweet: {example['text']} Label: {label}\"}\n","ds[\"train\"] = ds[\"train\"].map(lambda example: format_text(example, train=True))\n","ds[\"test\"] = ds[\"test\"].map(lambda example: format_text(example, train=False))\n","\n","#tokenize the data\n","def tok_func(x): return tokenizer(x[\"text\"])\n","tok_ds = ds.map(tok_func)\n","tok_ds = tok_ds.remove_columns(ds['train'].column_names)\n","\n","#set params\n","batch_size = 8\n","learning_rate=3e-4#2e-5\n","num_train_epochs=5\n","weight_decay=0.01\n","\n","#set up the data collator and loaders\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","train_dataloader = DataLoader(tok_ds['train'], shuffle=True, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n","eval_dataloader = DataLoader(tok_ds['test'], collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n","\n","#load optimizer and lr_scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_train_epochs),\n",")\n","\n","#do training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference! \n","for epoch in range(0,num_train_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"epoch: {epoch}, train_ppl: {train_ppl}, train_epoch_loss: {train_epoch_loss}\")\n","\n","#do eval\n","model.config.use_cache = True \n","def label_match(input_str):\n","    match = re.search(\" label\\s*:\\s*.*\", input_str.lower())  \n","    if match is not None:\n","        label_part = match.group()\n","        if \"label: not a real disaster\" in label_part:\n","            return 0\n","        elif \"label: real disaster\" in label_part:\n","            return 1\n","    return None  \n","\n","###run it on validation set\n","model.eval()\n","decoded_outputs = []\n","for i, batch in enumerate(tqdm(eval_dataloader)):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(**batch, pad_token_id=tokenizer.pad_token_id, max_new_tokens=20)\n","    \n","    outtexts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    decoded_outputs.extend([label_match(outtext) for outtext in outtexts])\n","\n","# prepare the data as a pandas DataFrame and output to csv\n","data = {\"id\": ds[\"test\"][\"id\"], \"input_text\": ds[\"test\"][\"text\"], \"predicted_label\": decoded_outputs, \"true_label\": ds[\"test\"][\"target\"]}\n","output_df = pd.DataFrame(data)\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_cln_eval_short.csv', index=False)\n","\n","###now do test\n","test_df = load_data(sep,aug=True,dset='test')\n","ds = Dataset.from_pandas(test_df)\n","ds = ds.map(lambda example: format_text(example, train=False))\n","tok_ds = ds.map(tok_func, batched=True, remove_columns=('keyword','id','location','text'))\n","test_dataloader = DataLoader(tok_ds, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n","decoded_outputs = []\n","for i, batch in enumerate(tqdm(test_dataloader)):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    \n","    with torch.no_grad():\n","        outputs = model.generate(**batch, pad_token_id=tokenizer.pad_token_id, max_new_tokens=20)\n","    \n","    outtexts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    decoded_outputs.extend([label_match(outtext) for outtext in outtexts])\n","\n","data = {\"id\": ds[\"id\"], \"input_text\": ds[\"text\"], \"predicted_label\": decoded_outputs}\n","output_df = pd.DataFrame(data)\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetuned_cln_test.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-19T18:03:38.869081Z","iopub.status.busy":"2023-06-19T18:03:38.868727Z","iopub.status.idle":"2023-06-19T18:15:44.730413Z","shell.execute_reply":"2023-06-19T18:15:44.728951Z","shell.execute_reply.started":"2023-06-19T18:03:38.869052Z"},"trusted":true},"outputs":[],"source":["#Embeddings + logreg classifier \n","def get_model_embeddings(texts, model, tokenizer, batch_size=8, encoder=False):\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","    dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    \n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.eval()  # Put the model in eval mode\n","\n","    embeddings = []\n","    with torch.no_grad():\n","        for i, (input_ids, attention_mask) in enumerate(data_loader):\n","            print(\"i=\",i)\n","            # Feed our sequences to the model\n","            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n","            \n","            mean_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n","            if encoder:\n","                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","            else:\n","                cls_embeddings = outputs.last_hidden_state[:, -1, :].cpu().numpy()\n","\n","            # Store both mean and last token embeddings\n","            embeddings.extend(list(zip(mean_embeddings, cls_embeddings)))\n","\n","    return embeddings\n","\n","def ClassifyEmbeddings(targets, embeddings_train, embeddings_eval, embeddings_test=None):\n","    from sklearn.linear_model import LogisticRegression\n","\n","    clf = LogisticRegression(max_iter=1000)\n","\n","    clf.fit(embeddings_train, np.array(targets))\n","\n","    y_val_pred = clf.predict(embeddings_eval)\n","    \n","    if embeddings_test is not None:\n","        y_test_pred = clf.predict(embeddings_test)\n","    else:\n","        y_test_pred = None\n","    return y_val_pred, y_test_pred\n","\n","#load and split the data\n","train_df = load_data(aug=True,dset='train')\n","ds = Dataset.from_pandas(train_df)\n","ds = ds.train_test_split(test_size=0.05, seed=42)\n","test_df = load_data(aug=True,dset='test')\n","test_ds = Dataset.from_pandas(test_df)\n","\n","model_name=\"roberta-large\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","embeddings = {}\n","embeddings['train'] = get_model_embeddings(ds['train']['text'], model, tokenizer, batch_size=16, encoder=True)\n","embeddings['eval'] = get_model_embeddings(ds['test']['text'], model, tokenizer, batch_size=16, encoder=True)\n","embeddings['test'] = get_model_embeddings(test_ds['text'], model, tokenizer, batch_size=16, encoder=True)\n","mean_embeddings_train, cls_embeddings_train = zip(*embeddings['train'])\n","mean_embeddings_eval, cls_embeddings_eval = zip(*embeddings['eval'])\n","mean_embeddings_test, cls_embeddings_test = zip(*embeddings['test'])\n","y_val_pred, y_test_pred = ClassifyEmbeddings(ds[\"train\"][\"target\"], cls_embeddings_train, cls_embeddings_eval, cls_embeddings_test)\n","output_df = pd.DataFrame({'id': ds['test']['id'],'input_text': ds['test'][\"text\"],'predicted_label': y_val_pred,\n","        'true_label': ds[\"test\"][\"target\"]})\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_cls_logreg_cln_eval_short.csv\", index=False)\n","output_df = pd.DataFrame({'id': test_ds['id'],'input_text': test_ds[\"text\"],'predicted_label': y_test_pred})\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_cls_logreg_cln_test.csv\", index=False)\n","\n","model = None\n","tokenizer=None\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","model_name=\"gpt2-xl\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n","embeddings = {}\n","embeddings['train'] = get_model_embeddings(ds['train']['text'], model, tokenizer, batch_size=16, encoder=False)\n","embeddings['eval'] = get_model_embeddings(ds['test']['text'], model, tokenizer, batch_size=16, encoder=False)\n","embeddings['test'] = get_model_embeddings(test_ds['text'], model, tokenizer, batch_size=16, encoder=False)\n","mean_embeddings_train, cls_embeddings_train = zip(*embeddings['train'])\n","mean_embeddings_eval, cls_embeddings_eval = zip(*embeddings['eval'])\n","mean_embeddings_test, cls_embeddings_test = zip(*embeddings['test'])\n","y_val_pred, y_test_pred = ClassifyEmbeddings(ds[\"train\"][\"target\"], cls_embeddings_train, cls_embeddings_eval, cls_embeddings_test)\n","output_df = pd.DataFrame({'id': ds['test']['id'],'input_text': ds['test'][\"text\"],'predicted_label': y_val_pred,\n","        'true_label': ds[\"test\"][\"target\"]})\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_last_logreg_cln_eval_short.csv\", index=False)\n","output_df = pd.DataFrame({'id': test_ds['id'],'input_text': test_ds[\"text\"],'predicted_label': y_test_pred})\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+\"_embeddings_aug_last_logreg_cln_test.csv\", index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#finetune encoder-decoder with lora - SEQ_2_SEQ_LM\n","!pip install peft\n","model_name=\"google/flan-t5-large\"#\"distilbert-base-uncased\"#\"cardiffnlp/twitter-roberta-base\"#\"roberta-base\"#\"bigscience/bloom-560m\"#\"roberta-large\"#\n","peft_model_id = \"cackerman/\"+model_name.rsplit('/', 1)[-1]+\"_aug_LORA_SEQ_2_SEQ_LM\"\n","from peft import get_peft_model, LoraConfig, TaskType, get_peft_config, get_peft_model_state_dict\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","import os\n","\n","peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, #SEQ_CLS\n","                         lora_dropout=0.1,target_modules=[\"q\", \"v\"])#for t5#)#, target_modules=[\"q_lin\", \"v_lin\"])#for distilbert (https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#L202)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)#, device_map='auto', torch_dtype=torch.float16)\n","model = get_peft_model(model, peft_config)\n","###model.print_trainable_parameters()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","if tokenizer.sep_token is None:\n","    tokenizer.sep_token = tokenizer.eos_token\n","    # resize model embedding to match new tokenizer\n","    model.resize_token_embeddings(len(tokenizer))\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","    model.config.pad_token_id = model.config.eos_token_id\n","    # resize model embedding to match new tokenizer\n","    model.resize_token_embeddings(len(tokenizer))\n","if \"gpt\" in model_name:\n","    tokenizer.padding_side = 'left'\n","\n","text_column = \"text\"\n","label_column = \"text_label\"\n","max_length = 128#64\n","lr = 3e-4\n","num_epochs = 3\n","batch_size = 8\n","\n","train_df = load_data(tokenizer.sep_token,True,'train')\n","ds = Dataset.from_pandas(train_df)\n","ds = ds.train_test_split(test_size=0.05, seed=42)\n","classes = ['Not a real disaster','Real disaster']\n","ds = ds.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"target\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","\n","def preprocess_function(examples):\n","    inputs = examples[text_column]\n","    targets = examples[label_column]\n","    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","    labels = tokenizer(targets, max_length=target_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","    labels = labels[\"input_ids\"]\n","    labels[labels == tokenizer.pad_token_id] = -100\n","    # Convert all torch.Tensors to lists or numpy arrays\n","    for key in model_inputs.keys():\n","        model_inputs[key] = model_inputs[key].numpy() # or .tolist()\n","    model_inputs[\"labels\"] = labels.numpy() # or .tolist()\n","    return model_inputs\n","\n","processed_datasets = ds.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=ds[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"test\"]\n","    \n","train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","                \n","###checkpoint = torch.load(save_path)\n","###model.load_state_dict(checkpoint[\"model_state_dict\"])\n","print(model.print_trainable_parameters())\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","for epoch in range(0,num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","#        if epoch==4 and step <= 704:\n","#          continue\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        # save model every save_steps\n","#        if step % save_steps == 0 and step > 0:\n","#          torch.save({\n","#              \"model_state_dict\": model.state_dict(),\n","#              \"optimizer_state_dict\": optimizer.state_dict(),\n","#              \"scheduler_state_dict\": lr_scheduler.state_dict()\n","#          }, save_path)\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"epoch: {epoch}, train_ppl: {train_ppl}, train_epoch_loss: {train_epoch_loss}, eval_ppl: {eval_ppl}, eval_epoch_loss: {eval_epoch_loss}\")\n","\n","correct = 0\n","total = 0\n","for pred, true in zip(eval_preds, ds[\"test\"][\"text_label\"]):\n","    if pred.strip() == true.strip():\n","        correct += 1\n","    total += 1\n","accuracy = correct / total * 100\n","print(f\"{accuracy} % on the evaluation dataset\")\n","print(f\"{eval_preds[:10]}\")\n","print(f\"{ds['test']['text_label'][:10]}\")\n","\n","import re\n","\n","###run on eval set\n","decoded_outputs = []\n","def label_mapping(input_str):\n","    if input_str == \"Not a real disaster\":\n","        return 0\n","    elif input_str == \"Real disaster\":\n","        return 1\n","    return\n","for i, batch in enumerate(eval_dataloader):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    input_ids = batch['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(input_ids=input_ids, max_new_tokens=10)\n","        outtext=tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n","        if i==0:\n","            print(\"inputs=\",tokenizer.batch_decode(input_ids.detach().cpu().numpy(), skip_special_tokens=True))\n","            print(\"outtext=\",outtext)\n","        decoded_outputs.extend(label_mapping(str) for str in outtext)\n","\n","targets = ds[\"test\"][\"target\"]\n","inputs = ds[\"test\"][\"text\"]\n","\n","data = {\"id\": ds[\"test\"][\"id\"], \"input_text\": inputs, \"predicted_label\": decoded_outputs, \"true_label\": targets}\n","output_df = pd.DataFrame(data)\n","\n","# Save the DataFrame to a CSV file\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetunedlora_classhead_cln_eval_short.csv', index=False)\n","\n","\n","###now do test set\n","test_df = load_data(tokenizer.sep_token,True,'test')\n","test_ds = Dataset.from_pandas(test_df)\n","def tok_func(x): return tokenizer(x[\"text\"], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","tok_ds = test_ds.map(tok_func)\n","tok_ds = tok_ds.remove_columns(test_ds.column_names)\n","test_dataloader = DataLoader(tok_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","\n","decoded_outputs = []\n","for i, batch in enumerate(test_dataloader):\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    input_ids = batch['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(input_ids=input_ids, max_new_tokens=10)\n","        outtext=tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n","        if i==0:\n","            print(\"inputs=\",tokenizer.batch_decode(input_ids.detach().cpu().numpy(), skip_special_tokens=True))\n","            print(\"outtext=\",outtext)\n","        decoded_outputs.extend(label_mapping(str) for str in outtext)\n","\n","data = {\"id\": test_ds[\"id\"], \"input_text\": test_ds['text'], \"predicted_label\": decoded_outputs}\n","output_df = pd.DataFrame(data)\n","output_df.to_csv(model_name.rsplit('/', 1)[-1]+'_aug_finetunedlora_classhead_cln_test.csv', index=False)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
